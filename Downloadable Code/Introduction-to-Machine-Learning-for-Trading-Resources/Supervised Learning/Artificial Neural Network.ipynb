{"cells":[{"cell_type":"markdown","metadata":{"id":"-EkaCVWo-u25"},"source":["# Notebook Instructions\n","\n","1. All the <u>code and data files</u> used in this course are available in the downloadable unit of the <u>last section of this course</u>.\n","2. You can run the notebook document sequentially (one cell at a time) by pressing **Shift + Enter**. \n","3. While a cell is running, a [*] is shown on the left. After the cell is run, the output will appear on the next line.\n","\n","This course is based on specific versions of Python packages. You can find the details of the packages in <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\" >this manual</a>.\n"]},{"cell_type":"markdown","metadata":{"id":"zU-74-FN-u28"},"source":["# Artificial Neural Networks\n","Artificial Neural Network (ANN) is a type of supervised machine learning. The name and the algorithm both are inspired by the human brain. Like neurons in our brains helps us process information similar to that node in ANN also helps to process information. An ANN aims to solve any specific problem in the same way as a human brain would. ANN consist of multiple nodes which mimic the biological neurons of a human brain. As they are connected through links, they interact by taking the data and performing operations on it and then passing it over to the other connected node.\n","\n","![title](https://d2a032ejo53cab.cloudfront.net/Glossary/wXKQi7H2/NN.png \"backtest\")\n","\n","Above is the visual representation of a neural network. \n","1. There is an input layer and an output layer. \n","1. Between the input and output layers, we have hidden layers. \n","1. We can have 1 to n number of hidden layers. \n","1. Each layer can have n number of nodes. \n","\n","This unit will talk about a specific type of neural network algorithm called multi-layer perceptrons (MLP). \n","\n","A multi-layer perceptron classifier (MLP) is a classifier that consists of multiple layers of nodes. Each layer is fully connected to the next layer in the network. Each link between the nodes has a certain weight. The algorithm adjusts the weights automatically based on the previous results. If the results are good then weights are not changed but if the results are not desirable then the algorithm alters the weights. Check this course on Quantra to learn more about <a href=\"https://quantra.quantinsti.com/course/neural-networks-deep-learning-trading-ernest-chan\" target=\"_blank\"> Neural networks</a> and how to use them for trading. \n","\n","In this notebook, you will perform the following steps:\n","\n","\n","1. [Independent and Dependent Variable](#x)\n","2. [MLP Classifier Model](#model)\n","3. [Make Prediction](#predict)\n","4. [Model Coefficients](#coff)\n","5. [Probabilty Estimation](#prob)"]},{"cell_type":"markdown","metadata":{"id":"3U399_IF-u28"},"source":["<a id='x'></a> \n","\n","## Independent and Dependent Variable\n","\n","Array X of size (n_samples, n_features), holds the training samples represented as floating-point feature vectors. Array y of size (n_samples) holds the target values (class labels) for the training samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgfzxGDf-u29","outputId":"e6f15bd4-4457-4c91-80a6-3883e340518b"},"outputs":[{"data":{"text/plain":["([[0.0, 0.0], [1.0, 1.0]], [0, 1])"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["X = [[0., 0.], [1., 1.]]\n","y = [0, 1]\n","X,y"]},{"cell_type":"markdown","metadata":{"id":"IKohBzDd-u2-"},"source":["<a id='model'></a> \n","## MLP Classifier Model\n","We will use the `MLPClassifier` function from sklearn to initialise the model and save it in the `cfl` variable. Next, we will call the `fit` method to train the model.\n","\n","Syntax:\n","```python\n","cfl.fit(X, y) \n","```\n","Parameters \n","\n","X: Explanatory variables in the training set\n","\n","y: Target variable in the training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do9Ktf0E-u2_","outputId":"c362c586-6f0b-4800-d7ba-308da20a2457"},"outputs":[{"data":{"text/plain":["MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n","              solver='lbfgs')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Import MLPClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","# Create the model\n","clf = MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\n","                    solver='lbfgs')\n","# Fit the model\n","clf.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"Mr3J8aIc-u2_"},"source":["<a id='predict'></a> \n","\n","## Make Prediction\n","After fitting (training), the model can predict labels for new samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiOcobI7-u3A","outputId":"38da8f0d-e000-4087-fc81-56560c5c4d3f"},"outputs":[{"data":{"text/plain":["array([1, 0])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["clf.predict([[2., 2.], [-1., -2.]])"]},{"cell_type":"markdown","metadata":{"id":"wPP-4Yoe-u3A"},"source":["<a id='coff'></a> \n","\n","## Model Coefficients\n","MLP can fit a non-linear model to the training data. `clf.coefs_` contains the weight matrices that constitute the model parameters. This is the same as in the Linear Regression Model, where we have betas for every independent variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THUcTqvN-u3A","outputId":"3f08e18a-86c4-4b9c-c7d1-1aa0698c3c74"},"outputs":[{"data":{"text/plain":["[(2, 5), (5, 2), (2, 1)]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["[coef.shape for coef in clf.coefs_]"]},{"cell_type":"markdown","metadata":{"id":"iY_Qelkk-u3B"},"source":["<a id='prob'></a> \n","\n","##  Probability Estimation\n","Currently, MLP Classifier supports only the Cross-Entropy loss function, allowing probability estimates by running the `predict_proba` method. MLP trains using Backpropagation. More precisely, it trains using gradient descent, and the gradients are calculated using Backpropagation. For classification, it minimizes the Cross-Entropy loss function, giving a vector of probability estimates per sample."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrtcEc0m-u3B","outputId":"425d5b37-f013-43ee-a192-857b6818c08c"},"outputs":[{"data":{"text/plain":["array([[1.96718015e-04, 9.99803282e-01],\n","       [1.96718015e-04, 9.99803282e-01]])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["clf.predict_proba([[2., 2.], [1., 2.]])"]},{"cell_type":"markdown","metadata":{"id":"WgvkZIQ6-u3B"},"source":["We can use the MLP classifiers to train for the particular sets of input variables and use the model to predict the outcomes, where the model uses the appropriate weights of the input layers to predict the outcome. If the results are in line with the training sample, then the weights are not changed. But if the outcome is not optimised, then the model changes each layer's weights and gives the desired result.<br><br>"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Artificial Neural Network.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}